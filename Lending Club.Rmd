---
title: "HW2"
output: html_document
date: "2022-11-01"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(lubridate)

df <- read_csv("Data mining/lcDataSampleFall22.csv",show_col_types = FALSE )
head(df)
glimpse(df)
```

```{r}
### 2a - What is the proportion of defaults (‘charged off’ vs ‘fully paid’ loans) in the data?
```{r}
### (i) How does default rate vary with loan grade? Does it vary with sub-grade? And is this what you would expect, and why?

df %>% count(loan_status) 
prop.table(table(df$loan_status))
# 86.2% in the data contains Fully paid and 13.7% contains Charged off

# relationship between Default rate and loan grade
table(df$loan_status,df$grade)
# Insights : Default(Charged off) gradually increases from grade A to C and then decreases

# relationship between Default rate and sub_grade
table(df$loan_status,df$sub_grade)
# insights : Defaults(Charged off) gradually increases from sub_grade A1 to C3 the it decreases 
#--------------------------------------------------------------------------------
  
# (ii) How many loans are there in each grade? And do loan amounts vary by grade? Does interest rate for loans vary with grade, subgrade? Look at the average, standard-deviation,min and max of interest rate by grade and subgrade. Is this what you expect, and why?

# Count of loans in each grade
df_loan_count <- df %>% group_by(grade) %>% count()
ggplot(df_loan_count, aes(x=grade, y =n)) + geom_col()

# loan amount vs grade
df_loan_amt <- df[c('grade','loan_amnt')] %>% group_by(grade) %>% summarize(mean_loan_amnt = mean(loan_amnt))
ggplot(df_loan_amt, aes(x=grade, y = mean_loan_amnt,fill=grade)) + geom_col()

# Interest rate vs grade
df_interset <- df[c('grade','sub_grade','int_rate')] %>% group_by(grade,sub_grade) %>% summarise(average = mean(int_rate),std_devition = sd(int_rate),minimum = min(int_rate),maximum=max(int_rate))

ggplot(df, aes(x=grade, y =int_rate,fill= sub_grade)) + geom_col()
#-----------------------------------------------------------------
  
#(iii) For loans which are fully paid back, how does the time-to-full-payoff vary? For this, calculate the ‘actual term’ (issue-date to last-payment-date) for all loans. How does this actual-term vary by loan grade (a box-plot can help visualize this). 

# Changing the columns to required datatypes
#issue_d is a date variable, while last_pymnt_d is of type character (like "Dec-2018", having month-year but no date). 
#We need to change the character type to date:
# For these loan we can set actual term at 3

#str(df$issue_d) <- as.Date((df[c("issue_d")]))

# Changing the datatype of last_pymt_d
df$last_pymnt_d<-paste(df$last_pymnt_d, "-01", sep = "")
df$last_pymnt_d<-parse_date_time(df$last_pymnt_d,  "myd")

# calculating actual term for 3 years
df$actual_term <- ifelse(df$loan_status=="Fully Paid", as.duration(df$issue_d  %--% df$last_pymnt_d)/dyears(1), 3)

# Boxplot for actual_term and grade
ggplot(df, aes(grade,actual_term)) + geom_boxplot() 
#------------------------------------------------------------------

# (iv) Calculate the annual return. Show how you calculate the percentage annual return.
#Is there any return from loans which are ‘charged off’? Explain. How does return from charged -off loans vary by loan grade?
#Compare the average return values with the average interest-rate on loans – do you notice anydifferences, and how do you explain this?
#How do returns vary by grade, and by sub-grade.If you wanted to invest in loans based on this data exploration, which loans would you invest in?

df$actual_return <- ifelse(df$actual_term>0, ((df$total_pymnt-df$funded_amnt)/df$funded_amnt)*(1/df$actual_term)*100, 0)

df %>% select(loan_status, int_rate, funded_amnt, total_pymnt, actual_term, actual_return) %>%  head()

# Charged off loan vs grade using Boxplot
df_loangrade_return <- df %>% filter(loan_status=="Charged Off") %>% select(grade,actual_return)
ggplot(df_loangrade_return,aes(x= grade, y = actual_return)) + geom_boxplot()

#(vi) Consider some borrower characteristics like employment-length, annual-income, fico-scores(low, high). How do these relate to loan attributes like, for example, loan_amout, loan_status,grade, purpose, actual return, etc. 

df %>% group_by(emp_length) %>% tally()

# Converting emp_length to factor
df$emp_length <- factor(df$emp_length, levels=c("n/a", "< 1 year","1 year","2 years", "3 years" ,  "4 years",   "5 years",   "6 years",   "7 years" ,  "8 years", "9 years", "10+ years" ))

# Calculating defaults, default_rate, average interest rate with emp_length
df %>% group_by(emp_length) %>% summarise(nLoans=n(), defaults=sum(loan_status=="Charged Off"), default_rate=defaults/nLoans, avg_int_rate=mean(int_rate),  avg_loan_amt=mean(loan_amnt),  avg_act_ret = mean(actual_return), avg_act_term=mean(actual_term))

#emp_length vs grade
df_emp_grade <- df %>% group_by(emp_length) %>% count(grade)
ggplot(df_emp_grade, aes(x=emp_length,y=n,fill=grade)) + geom_col() + theme(axis.text.x = element_text(angle = 90))

# emp_length vs loan_status
df_emp_status <- df %>% group_by(emp_length) %>% count(loan_status)
df_emp_status
ggplot(df_emp_status, aes(x=emp_length,y=n,fill=loan_status)) + geom_col() + theme(axis.text.x = element_text(angle = 90)) 

# annual income vs loan_status and grade
df_inc_grade <- df %>% group_by(loan_status,grade) %>% summarise(avg_income = mean(annual_inc))
ggplot(df_inc_grade, aes(x=grade, y=avg_income, fill=loan_status)) + geom_col()
#--------------------------------------------------------------

# (vii) Generate some (at least 3) new derived attributes which you think may be useful for predicting default., and explain what these are. For these, do an analyses as in the questions above (as reasonable based on the derived variables).

#Derived attribute: proportion of satisfactory bankcard accounts 
df$propSatisBankcardAccts <- ifelse(df$num_bc_tl>0, df$num_bc_sats/df$num_bc_tl, 0)

# calculate the length of borrower's history with LC
#  i.e time between earliest_cr_line and issue_d
df$earliest_cr_line<-paste(df$earliest_cr_line, "-01", sep = "")
df$earliest_cr_line<-parse_date_time(df$earliest_cr_line, "myd")

df$borrHistory <- as.duration(df$earliest_cr_line %--% df$issue_d  ) / dyears(1)

#Another new attribute: ratio of openAccounts to totalAccounts
df$openAccRatio <- df$open_acc/df$total_acc

#LC-assigned loan grade vary by borrHistory?
df %>% group_by(grade) %>% summarise(avgBorrHist=mean(borrHistory),avgloanamt = mean(loan_amnt), avgintrate = mean(int_rate))

# loan status vary by borrower's histroy
df %>% group_by(loan_status) %>% summarise(avgBorrHist=mean(borrHistory))

#LC-assigned loan grade vary by openAccRatio?
df %>% group_by(grade) %>% summarise(avgopenAccRatio = mean(openAccRatio))

#LC-assigned loan grade vary by satisfactory bankcard accounts?
df %>% group_by(grade) %>% summarise(avgpropSatisBankcardAccts = mean(propSatisBankcardAccts))

```
```
```{r}
# (b) Are there missing values? What is the proportion of missing values in different variables?Explain how you will handle missing values for different variables. You should consider what he variable is about, and what missing values may arise from – for example, a variable monthsSinceLastDeliquency may have no value for someone who has not yet had a delinquency;
# what is a sensible value to replace the missing values in this case?
# Are there some variables you will exclude from your model due to missing values?
```{r}
#To find missing values using colSums
colSums(is.na(df))

# There are few variables which are completely null so we are removing those
df_final <- df %>% select_if(function(x){ ! all(is.na(x))})
dim(df_final)
# Removed 37 variables which had 100% missing values

# To find the names of the columns with mising values
names(df_final)[colSums(is.na(df_final)) > 0]

#To find percentage of missing values in each column
colMeans(is.na(df_final))* 100

# There are 4 ways we can handle missing values
#1. Remove rows
#2. Replace with mean or median
#3. Replace with range limit
#4. Replace with other values based on domain language or data set knowledge

#We can replace missing values in a variable with replace_na()
df_final$open_acc_6m <- as.character(df_final$open_acc_6m)
replace_na( df_final$open_acc_6m, "missing") 

df_mths_since_delinq[1,]/(df_mths_since_delinq[2,]+df_mths_since_delinq[1,])
   #Here,there is a pattern of higher defaults for examples which have more recent delinquencies.We should try to retain this variable, and find a way to reasonably handle the missing values.

#For mths_since_recent_inq, which has around 10% values missing
df_mths_since_inq<-table( df_final$loan_status, replace_na( as.character(df_final$mths_since_recent_inq), "missing") )
df_mths_since_inq[1,]/(df_mths_since_inq[2,]+df_mths_since_inq[1,])
    # Here,the proportion of defaults for missing values seem similar to the larger values of the variable -- so, may be replace the missings with a large value 

#Suppose you decide to remove variables which have more than 60% missing values
nm<-names(df_final)[colMeans(is.na(df_final))>0.9]
df_final <- df_final %>% select(-all_of(nm))
# Here we removed 20 columns

# Replacing the missing value with the median of that column
df_final<- df_final %>% replace_na(list(mths_since_last_delinq=-500, bc_open_to_buy=median(df_final$bc_open_to_buy, na.rm=TRUE), mo_sin_old_il_acct=1000, mths_since_recent_bc=1000, mths_since_recent_inq=50, num_tl_120dpd_2m = median(df_final$num_tl_120dpd_2m, na.rm=TRUE),percent_bc_gt_75 = median(df_final$percent_bc_gt_75, na.rm=TRUE), bc_util=median(df_final$bc_util, na.rm=TRUE)))


#Finding the summary ofall the varibles
nm1<- names(df_final)[colMeans(is.na(df_final))>0]
# we can see that we have removed all the missing values
summary(df_final)
glimpse(df_final %>% select(nm1))
```

```{r}
#(c) Consider the potential for data leakage. You do not want to include variables in your model which may not be available when applying the model; that is, some data may not be available for new loans before they are funded. Leakage may also arise from variables in the data which may have been updated during the loan period (ie., after the loan is funded). Identify and explain which variables will you exclude from the model.

```{r}
#Drop some variable/columns which are not useful or which we will not use in developing predictive models
#Identify the variables you want to remove

varsToRemove <- c('funded_amnt', 'funded_amnt_inv', 'emp_title','issue_d','zip_code','out_prncp', 'out_prncp_inv', 'total_pymnt_inv','total_rec_prncp', 'total_rec_int', 'total_rec_late_fee','recoveries', 'collection_recovery_fee', 'last_pymnt_amnt','term', 'last_credit_pull_d', 'policy_code', 'disbursement_method', 'debt_settlement_flag', 'application_type','title',"emp_length")


# The funded_amnt and funded_amnt_inv are both features about the future the loan has been approved at that point and cannot be considered in our model.
#the emp_title feature would be a hard feature to evaluate so we are removing it
#The following 5 variables are all about the future, they inform us about how the repayment is going, out_prncp, out_prncp_inv, total_pymnt, total_pymnt_inv, total_rec_prncp. We need to remove them from our model. 

#Drop them from the lcdf data-frame
df_final <- df_final %>% select(-all_of(varsToRemove))  

#Drop all the variables with names starting with "hardship"
df_final <- df_final %>% select(-starts_with("hardship"))

#similarly, all variable starting with "settlement"
df_final <- df_final %>% select(-starts_with("settlement"))

```

```{r}
#Converting character variables
```{r}
#  notice that there are a few character type variables - grade, sub_grade, verification_status,....
#   We can  convert all of these to factor
df_final <- df_final %>% mutate_if(is.character, as.factor)

```


#3. Do a univariate analyses to determine which variables (from amongst those you decide to consider for the next stage prediction task) will be individually useful for predicting the dependent variable (loan_status). 
#For this, you need a measure of relationship between the dependent variable and each of the potential predictor variables. 
#Given loan-status as a binary dependent variable, which measure will you use? From your analyses using this measure, which variables do you think will be useful for predicting loan_status?
```{r}
### Univariate analysis - Finding the summary statistics of each column
#structure of dataframe
str(df_final)

#Findings : There are 69 fields with 100000 rows in train dataset

#summary
summary(df_final)

# Boxplot for all the variables
#par(mfrow = c(1, ncol(df_final)),mar=c(1,1,1,1))
invisible(lapply(1:ncol(df_final), function(i) boxplot(df_final[, i])))
#------------------------------------------
```

```{r}
##We will next develop predictive models for loan_status.
#4. (a) Split the data into training and validation sets. What proportions do you consider, why?
```{r}
# Splitting the dataset into train and test using random sample
#Empirical studies show that the best results are obtained if we use 20-30% of the data for testing, and the remaining 70-80% of the data for training.
# we are considering 70-30 propotions
df_final$loan_status <- as.factor(df_final$loan_status)
library(caret)
set.seed(3456)
sample <- createDataPartition(df_final$loan_status, p = .7,list = FALSE, times = 1)

df_train <- df_final[ sample,]
df_test <- df_final[-sample,]
```

```{r}
# 5. Develop a decision tree model to predict default.
#Train decision tree models (use either rpart or c50)
#What parameters do you experiment with, and what performance do you obtain (on training and validation sets)? Clearly tabulate your results and briefly describe your findings.
```{r}
glimpse(df_train)

varsOmit <- c('actual_term', 'actual_return', 'propSatisBankcardAccts', 'openAccRatio','last_pymnt_d') 

library(rpart)

DT <- rpart(loan_status ~., data=df_train %>% select(-all_of(varsOmit)), method="class", parms = list(split = "information"), control = rpart.control(minsplit = 30))
printcp(DT)  #reasonable ?  (If the tree does not grow at all, maybe set a lower value of cp?)

#variable importance
DT$variable.importance

# Changing hyper parameters
DT <- rpart(loan_status ~., data=df_train %>% select(-all_of(varsOmit)), method="class", parms = list(split = "information"), control = rpart.control(cp=0.0001, minsplit = 50))

library(rpart.plot)
rpart.plot(DT, main="Decision Tree")


#prune the tree -- check for performance with different cp levels
printcp(DT)
DT1<- prune.rpart(DT, cp=0.00022)   

#......

#Training the model considering a more balanced training dataset
             
DTb<- rpart(loan_status ~., data=df_train %>% select(-all_of(varsOmit)), method="class",  parms =list(split = "gini", prior=c(0.5, 0.5)), control = rpart.control(cp=0.0, minsplit = 20, minbucket = 10, maxdepth = 20,  xval=10))

# Decision tree model evaulation

# Predicting the model on train data
df_table_train <-table(pred = predict(DT1,df_train, type='class'), true=df_train$loan_status)
df_table_train
mean(predict(DT1,df_train, type='class') ==df_train$loan_status)
# Findings : Accuracy is 86.5%

#Confusion Matrix for train data
confusionMatrix(df_table_train,reference = df_train$loan_status)
#Findings :
# 1. Accuracy of the decision tree on training data is 86.5%
# 2. Sensitivity of the decision tree on training data is 0.067
# 3. Specificity of the decision tree on training data is 0.99

# Predicting the model on test data
df_table_test <- table(pred = predict(DT1,df_test, type='class'), true=df_test$loan_status)
df_table_test
mean(predict(DT1,df_test, type='class') ==df_test$loan_status)
# Findings : Accuracy is 85.5%

#Confusion Matrix for test data
confusionMatrix(df_table_test,reference = df_test$loan_status)
#Findings :
# 1. Accuracy of the decision tree on training data is 85.5%
# 2. Sensitivity of the decision tree on training data is 0.036
# 3. Specificity of the decision tree on training data is 0.989

library(ROCR)
# Function to find lift, ROC curve and AUC value
fnROCPerformance <- function(scores, data) {pred=prediction(scores, data$loan_status, label.ordering = c("Charged Off", "Fully Paid" ))

  #ROC curve
  aucPerf <-performance(pred, "tpr", "fpr")
  plot(aucPerf)
  abline(a=0, b= 1)

  #AUC value
  aucPerf=performance(pred, "auc")
  sprintf("AUC: %f", aucPerf@y.values)
  
  #Lift curve
  liftPerf <-performance(pred, "lift", "rpp")
  plot(liftPerf)
        
}
fnROCPerformance(score=predict(DT1,df_test, type="prob")[,"Charged Off"],df_test)
# AUC : 0.35

# Decile lift performance, for minority class (Charged Off") 
fnDecileLiftsPerformance_defaults  <- function( scores, dat) {
  totDefRate= sum(dat$loan_status=="Charged Off")/nrow(dat)
  decPerf <- data.frame(scores)
  decPerf <- cbind(decPerf, status=dat$loan_status, grade=dat$grade)
  decPerf <- decPerf %>% mutate(decile = ntile(-scores, 10))
  decPerf<-  decPerf  %>% group_by(decile) %>% summarise ( 
    count=n(), numDefaults=sum(status=="Charged Off"), defaultRate=numDefaults/count,
    totA=sum(grade=="A"),totB=sum(grade=="B" ), totC=sum(grade=="C"), totD=sum(grade=="D"),
    totE=sum(grade=="E"),totF=sum(grade=="F") )
  decPerf$cumDefaults=cumsum(decPerf$numDefaults)                      
  decPerf$cumDefaultRate=decPerf$cumDefaults/cumsum(decPerf$count)                      
  decPerf$cumDefaultLift<- decPerf$cumDefaultRate/(sum(decPerf$numDefaults)/sum(decPerf$count))
  
  print(decPerf)
}

fnDecileLiftsPerformance_defaults(score=predict(DT1,df_test, type="prob")[,"Charged Off"],df_test)

```

```{r}
# 6 question
# Random forest Model
```{r}
library(ranger)
library(caret)

TRNFRACTION = 0.7
#Doing a 70-30 split between training and test subsets
nr<-nrow(df_final)

trnIndex<- sample(1:nr, size = round(TRNFRACTION * nr), replace=FALSE)
rfTrn <- df_final[trnIndex, ]
rfTst <- df_final[-trnIndex, ]

rgModel1 <- ranger(as.factor(loan_status)~., data=rfTrn %>%  select(-all_of(varsOmit)),num.trees =50,importance='permutation')

# Predicting the random forest model on train data
scoreTrn <- predict(rgModel1,rfTrn)
a<-table(rfTrn$loan_status,predictions(scoreTrn))
confusionMatrix(a)
rgModel1$prediction.error
# Accuracy : 99.9% on the train data

# Predicting the random forest model on test data
scoreTst <- predict(rgModel1,rfTst)
a<-table(rfTst$loan_status,predictions(scoreTst))
confusionMatrix(a)
rgModel1$prediction.error


# To find the ROC,AUC and lift plot
fnROCPerformance(predict(rgModel3,rfTst)$predictions[,"Fully Paid"], rfTst)

#for decile defaults-lift performance
fnDecileLiftsPerformance_defaults( predict(rgModel3,rfTst)$predictions[,"Charged Off"], rfTst)
```

```{r}
library(xgboost)
#Needs all data to be numeric -- so we convert categorical (i.e. factor) variables using one-hot encoding – multiple ways to do this
sapply(lapply(df_final, unique), length)

df_xg <- df_final %>% select(-pymnt_plan,initial_list_status)

# use the dummyVars function in the 'caret' package to convert factor variables to # dummy-variables
fdum <-dummyVars(~.,data=df_xg %>% select(-loan_status)) #do not include loan_status for this
dxlcdf <- predict(fdum, df_xg)
# for loan_status, check levels and convert to dummy vars and keep the class label of interest
#levels(lcdf$loan_status)
dylcdf <- class2ind(as.factor(df_xg$loan_status), drop2nd = FALSE)
# and then decide which one to keep
fplcdf <- dylcdf [ , 1] # or, 
colcdf <- dylcdf [ , 2]
#Training, test subsets
dxlcdfTrn <- dxlcdf[trnIndex,]
colcdfTrn <- colcdf[trnIndex]
dxlcdfTst <- dxlcdf[-trnIndex,]
colcdfTst <- colcdf[-trnIndex]
dxTrn <- xgb.DMatrix(dxlcdfTrn,label=colcdfTrn)
dxTst <- xgb.DMatrix(dxlcdfTst,label=colcdfTst)
#dxTst <- xgb.DMatrix( subset( dxlcdfTst, select = -c( actual_term, actual_return, total_pymnt)), label=colcdfTst)
xgbWatchlist <- list(train = dxTrn, eval = dxTst)
#we can watch the progress of learning thru performance on these datasets
#list of parameters for the xgboost model development functions
xgbParam <- list (
max_depth = 5, eta = 0.01,
objective = "binary:logistic",
eval_metric="error", eval_metric = "auc")
#can specify which evaluation metrics we want to watch
xgb_lsM1 <- xgb.train( xgbParam, dxTrn, nrounds = 500,
xgbWatchlist, early_stopping_rounds = 10 )
xgb_lsM1$best_iteration
xpredTrg<-predict(xgb_lsM1, dxTrn)
head(xpredTrg) 

xpredTst<-predict(xgb_lsM1, dxTst)
pred_xgb_lsM1=prediction(xpredTst,rfTst$loan_status,label.ordering = c("Fully Paid", "Charged Off"))
aucPerf_xgb_lsM1=performance(pred_xgb_lsM1, "tpr", "fpr")
plot(aucPerf_xgb_lsM1)
abline(a=0, b= 1)
plot.new()
#use cross-validation on training dataset to determine best model
xgbParam <- list (
max_depth = 3, eta = 0.1,
objective = "binary:logistic",
eval_metric="error", eval_metric = "auc")
xgb_lscv <- xgb.cv( xgbParam, dxTrn, nrounds = 500, nfold=5, early_stopping_rounds = 10 )
#best iteration
xgb_lscv$best_iteration
# or for the best iteration based on performance measure (among those specified in xgbParam)
best_cvIter <- which.max(xgb_lscv$evaluation_log$test_auc_mean)
#which.min(xgb_lscv$evaluation_log$test_error_mean)
#best model
xgb_lsbest <- xgb.train( xgbParam, dxTrn, nrounds = xgb_lscv$best_iteration )
#variable importance
xgb.importance(model = xgb_lsbest) %>% view()

xgbParamGrid <- expand.grid(
max_depth = c(2, 5),
eta = c(0.001, 0.01, 0.1) )
xgbParamGrid

xgbParam <- list (
booster = "gbtree",
objective ="binary:logistic",
#eta=0.01, #learning rate
#max_depth=5,
min_child_weight=1,
colsample_bytree=0.6
)

for(i in 1:nrow(xgbParamGrid)) {
xgb_tune<- xgb.train(data=dxTrn,xgbParam,
nrounds=1000, early_stopping_rounds = 10, xgbWatchlist,
eta=xgbParamGrid$eta[i], max_depth=xgbParamGrid$max_depth[i] )
xgbParamGrid$bestTree[i] <- xgb_tune$evaluation_log[xgb_tune$best_iteration]$iter
xgbParamGrid$bestPerf[i] <- xgb_tune$evaluation_log[xgb_tune$best_iteration]$eval_auc
}

# ROC Curve
perfRoc_xgbTst=aucPerf_xgb_lsM1
plot(perfRoc_xgbTst,col='purple')
```


```{r}
TRNFRACTION = 0.7
#Doing a 70-30 split between training and test subsets
num_row<-nrow(temp)

trn<- sample(1:num_row, size = round(TRNFRACTION * num_row), replace=FALSE)
lcdfTrn2 <- temp[trn, ]
lcdfTst2 <- temp[-trn, ]

#Random forest Model - num.trees=50 and importance=permutation
rf <- ranger(as.factor(loan_status) ~., data=subset(lcdfTrn2, select=-c(actualTerm, actualReturn, total_pymnt,emp_title,last_pymnt_d,last_credit_pull_d,title, avg_cur_bal, num_rev_accts, pct_tl_nvr_dlq)),num.trees =50, importance='permutation', probability = TRUE)
rfPredictions <- predict(rf, lcdfTst2)$predictions
scoreRF <- rfPredictions[, "Fully Paid"]
prPerfRF <- data.frame(scoreRF)
prRetPerfRF <- cbind(prPerfRF, status=lcdfTst2$loan_status, grade=lcdfTst2$grade, actRet=lcdfTst2$actualReturn, actTerm = lcdfTst2$actualTerm)
prRetPerfRF <- prRetPerfRF %>% mutate(decile = ntile(-scoreRF, 10))
view(prRetPerfRF)
prRetPerfRF %>% group_by(decile) %>% summarise(count=n(), numDefaults=sum(status=="Charged Off"), avgActRet=mean(actRet),
minRet=min(actRet), maxRet=max(actRet), avgTer=mean(actTerm), totA=sum(grade=="A"), totB=sum(grade=="B" ), totC=sum(grade=="C"),
totD=sum(grade=="D"), totE=sum(grade=="E"), totF=sum(grade=="F") )
#Performance
prPerfRF2 <- cbind(prPerfRF, status=lcdfTst2$loan_status)
prPerfRF2 <- prPerfRF2[order(-scoreRF) ,]
prPerfRF2$profit <- ifelse(prPerfRF2$status == 'Fully Paid', PROFITVAL, COSTVAL)
prPerfRF2$cumProfit <- cumsum(prPerfRF2$profit)
view(prPerfRF2)
max(prPerfRF2$cumProfit)
which.max(prPerfRF2$cumProfit)
plot(prPerfRF2$cumProfit)
#to compare against the default approach of investing in CD with 2% int
# (ie. $6 profit out of $100 in 3 years)
prPerfRF2$cdRet <-6
prPerfRF2$cumCDRet<- cumsum(prPerfRF2$cdRet)
plot(prPerfRF2$cumProfit)
lines(prPerfRF2$cumCDRet, col='red')

TRNFRACTION = 0.7
#Doing a 70-30 split between training and test subsets
num_row<-nrow(temp)

trn<- sample(1:num_row, size = round(TRNFRACTION * num_row), replace=FALSE)
lcdfTrn2 <- temp[trn, ]
lcdfTst2 <- temp[-trn, ]  
vasr <- c('emp_title','title','zip_code','earliest_cr_line','last_credit_pull_d')
lcdfTrn2 <- lcdfTrn2 %>% select(-all_of(vasr))
lcdfTst2 <- lcdfTst2 %>% select(-all_of(vasr))

#Using R-part decision tree
rf <- rpart(loan_status ~., data=lcdfTrn2, method="class", parms = list(split = "gini"), control=rpart.control(cp=0.00036,minsplit =30))
rfPredictions <- predict(rf, lcdfTst2)
scoreRF <- rfPredictions[, "Fully Paid"]
prPerfRF <- data.frame(scoreRF)
prRetPerfRF <- cbind(prPerfRF, status=lcdfTst2$loan_status, grade=lcdfTst2$grade, actRet=lcdfTst2$actualReturn, actTerm = lcdfTst2$actualTerm)
prRetPerfRF <- prRetPerfRF %>% mutate(decile = ntile(-scoreRF, 10))
view(prRetPerfRF)
prRetPerfRF %>% group_by(decile) %>% summarise(count=n(), numDefaults=sum(status=="Charged Off"), avgActRet=mean(actRet),
minRet=min(actRet), maxRet=max(actRet), avgTer=mean(actTerm), totA=sum(grade=="A"), totB=sum(grade=="B" ), totC=sum(grade=="C"),
totD=sum(grade=="D"), totE=sum(grade=="E"), totF=sum(grade=="F") )
#Performance
prPerfRF3 <- cbind(prPerfRF, status=lcdfTst2$loan_status)
prPerfRF3 <- prPerfRF2[order(-scoreRF) ,]
prPerfRF3$profit <- ifelse(prPerfRF3$status == 'Fully Paid', PROFITVAL, COSTVAL)
prPerfRF3$cumProfit <- cumsum(prPerfRF3$profit)
view(prPerfRF3)
max(prPerfRF3$cumProfit)
which.max(prPerfRF3$cumProfit)
plot(prPerfRF3$cumProfit)
#to compare against the default approach of investing in CD with 2% int
# (ie. $6 profit out of $100 in 3 years)
prPerfRF3$cdRet <-6
prPerfRF3$cumCDRet<- cumsum(prPerfRF3$cdRet)
plot(prPerfRF3$cumProfit)
lines(prPerfRF3$cumCDRet, col='red')
```

